{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/e2eAIOK/blob/main/demo/ma/domain_adapter/Model_Adapter_Domain_Adapter_builtin_Unet_KITS19.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "baca96ea",
      "metadata": {},
      "source": [
        "# Content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2defe46a",
      "metadata": {},
      "source": [
        "- 1. [Overview](#toc1_)    \n",
        "  - 1.1. [Model Adapter Domain Adapter Overview](#toc1_1_)    \n",
        "- 2. [Getting Started](#toc2_)    \n",
        "  - 2.1. [Environment Setup](#toc2_1_)    \n",
        "    - 2.1.1. [(Option 1) Use Pip install - recommend](#toc2_1_1_)    \n",
        "    - 2.1.2. [(Option 2) Use Docker](#toc2_1_2_)    \n",
        "  - 2.2. [Workflow Prepare](#toc2_2_)    \n",
        "  - 2.3. [Data Prepare](#toc2_3_)    \n",
        "  - 2.4. [Launch Training](#toc2_4_)    \n",
        "  - 2.5. [Launch Inference & Evaluation](#toc2_5_)    \n",
        "  - 2.6. [Prediction Visualization](#toc2_6_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=true\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08261c5a",
      "metadata": {},
      "source": [
        "# 1. <a id='toc1_'></a>[Overview](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d30c03a",
      "metadata": {},
      "source": [
        "## 1.1. <a id='toc1_1_'></a>[Model Adapter Domain Adapter Overview](#toc0_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "50f19121",
      "metadata": {
        "id": "50f19121"
      },
      "source": [
        "Model Adapter is a convenient framework can be used to reduce training and inference time, or data labeling cost by efficiently utilizing public advanced models and those datasets from many domains. It mainly contains three components served for different cases: Finetuner, Distiller, and Domain Adapter. \n",
        "\n",
        "Directly applying pre-trained model into target domain cannot always work due to covariate shift and label shift, while fine-tuning is also not working due to the expensive labeling in some domains. Even if users invest resource in labeling, it will be time-consuming and delays the model deployment.\n",
        "\n",
        "Domain Adapter aims at reusing the transferable knowledge with the help of another labeled dataset with same learning task. That is, achieving better generalization with little labeled target dataset or achieving a competitive performance in label-free target dataset.\n",
        "\n",
        "The following picture show the network strcture of domain adaption, which add a discriminator to users' base network, and try to differentiate the souce domain data and target domain data, hence, it can force the feature extractor to learn a generalized feature representation among domains.\n",
        "\n",
        "![Adapter](../imgs/adapter.png)\n",
        "\n",
        "In this demo, we will introduce how to use Domain Adapter to transfer knowledge in medical image semantic segmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8873794",
      "metadata": {},
      "source": [
        "# 2. <a id='toc2_'></a>[Getting Started](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd231322",
      "metadata": {},
      "source": [
        "## 2.1. <a id='toc2_1_'></a>[Environment Setup](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac173d9",
      "metadata": {},
      "source": [
        "### 2.1.1. <a id='toc2_1_1_'></a>[(Option 1) Use Pip install - recommend](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ba7058",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install e2eAIOK-ModelAdapter --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27afc903",
      "metadata": {},
      "source": [
        "### 2.1.2. <a id='toc2_1_2_'></a>[(Option 2) Use Docker](#toc0_)\n",
        "\n",
        "Step1. prepare code\n",
        "   ``` bash\n",
        "   git clone https://github.com/intel/e2eAIOK.git\n",
        "   cd e2eAIOK\n",
        "   git submodule update --init –recursive\n",
        "   ```\n",
        "    \n",
        "Step2. build docker image\n",
        "   ``` bash\n",
        "   python3 scripts/start_e2eaiok_docker.py -b pytorch112 --dataset_path ${dataset_path} -w ${host0} ${host1} ${host2} ${host3} --proxy  \"http://addr:ip\"\n",
        "   ```\n",
        "   \n",
        "Step3. run docker and start conda env\n",
        "   ``` bash\n",
        "   sshpass -p docker ssh ${host0} -p 12347\n",
        "   conda activate pytorch-1.12.0\n",
        "   ```\n",
        "  \n",
        "Step4. Start the jupyter notebook and tensorboard service\n",
        "   ``` bash\n",
        "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/e2eaiok --ip=${hostname} --port=8899 --allow-root &\n",
        "   nohup tensorboard --logdir /home/vmagent/app/data/tensorboard --host=${hostname} --port=6006 & \n",
        "   ```\n",
        "   Now you can visit demso in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eafe6ac9",
      "metadata": {},
      "source": [
        "## 2.2. <a id='toc2_2_'></a>[Workflow Prepare](#toc0_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3221d99f",
      "metadata": {},
      "source": [
        "- First Clone the demo code\n",
        "    ``` bash\n",
        "    git clone https://github.com/intel/e2eAIOK.git\n",
        "    cd e2eAIOK\n",
        "    git submodule update --init –recursive\n",
        "    ```\n",
        "- Then add a patch to that code to get the full demo code\n",
        "    ```bash\n",
        "    cd modelzoo/unet && sh patch_unet.sh \n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "801223ad",
      "metadata": {},
      "source": [
        "## 2.3. <a id='toc2_3_'></a>[Data Prepare](#toc0_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e1c3633b",
      "metadata": {},
      "source": [
        "* Our source domain is AMOS dataset(Download AMOS data from [here](https://amos22.grand-challenge.org/Dataset/)), which provides 500 CT and 100 MRI scans with voxel-level annotations of 15 abdominal organs, including the spleen, right kidney, left kidney, gallbladder, esophagus, liver, stomach, aorta, inferior vena cava, pancreas, right adrenal gland, left adrenal gland, duodenum, bladder, prostate/uterus.\n",
        "* Our target domain is KiTS dataset(Download KiTS data from [here](https://github.com/neheller/kits19)), which provides 300 CT scans with voxel-level annotations of kidney organs and kidney tumor.\n",
        "* After downloading the code, remember to put all your data in right places, now your files should be located at:\n",
        "   - Images at: ```${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/imagesTr/```\n",
        "   - Labels/Segmentations at: ```${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/labelsTr/```\n",
        "   - Please refer to [here](https://github.com/MIC-DKFZ/nnUNet) to know how to put all your data in your `${dataset_path}` in right format.\n",
        "* Our task is to explore reliable kidney semantic segmentation methodologies with the help of labeled AMOS dataset and unlabeled KiTS dataset, evalutaion metric is kidney dice score in target domain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ebf186",
      "metadata": {},
      "source": [
        "## 2.4. <a id='toc2_4_'></a>[Launch Training](#toc0_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a67fdf64",
      "metadata": {
        "id": "a67fdf64"
      },
      "source": [
        "- We take [3D-UNet](https://arxiv.org/abs/1606.06650) as users' base model\n",
        "- We will first pre-train model in AMOS dataset, and use this pre-trained model later for prameter initialization for domain adaptation\n",
        "- Then we apply domain adaptation algorithm to transfer knowledge from AMOS dataset to KiTS dataset\n",
        "    - We use a DANN-like model architecture, the DANN algorithm is illustrated as follows:\n",
        "    ![dann](../imgs/dann.png)\n",
        "- Notice: \n",
        "    - we donot use **any label** from target domain KiTS, we only use label from source domain AMOS for training\n",
        "    - *For demostration, we only train 1 epochs:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b92e628",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+ unset MASTER_ADDR\r\n",
            "+ echo ############################## setting env ##############################\r\n",
            "############################## setting env ##############################\r\n",
            "+ export nnUNet_raw_data_base=/home/vmagent/app/data/adaptor_large/nnUNet_raw_data_base\r\n",
            "+ export nnUNet_preprocessed=/home/vmagent/app/data/adaptor_large/nnUNet_preprocessed\r\n",
            "+ export RESULTS_FOLDER=/home/vmagent/app/data/adaptor_large/nnUNet_trained_models\r\n",
            "+ pre_trained_model_path=/home/vmagent/app/data/adaptor_large/pre-trained-model/model_final_checkpoint-600.model\r\n",
            "+ echo ############################## 1 node opt model ##############################\r\n",
            "############################## 1 node opt model ##############################\r\n",
            "+ epochs=1\r\n",
            "+ nnUNet_train_da 3d_fullres nnUNetTrainer_DA_V2 508 507 1 -p nnUNetPlansv2.1_trgSp_kits19 -sp nnUNetPlansv2.1_trgSp_kits19 --epochs 1 --loss_weights 1 0 1 0 0 -pretrained_weights /home/vmagent/app/data/adaptor_large/pre-trained-model/model_final_checkpoint-600.model\r\n",
            "scripts/run_single_opt.sh: 34: nnUNet_train_da: not found\r\n"
          ]
        }
      ],
      "source": [
        "! cd modelzoo/unet && sh scripts/run_single_opt.sh 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db5681ed",
      "metadata": {},
      "source": [
        "## 2.5. <a id='toc2_5_'></a>[Launch Inference & Evaluation](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ba19a5d",
      "metadata": {
        "id": "4ba19a5d"
      },
      "source": [
        "* We use following command for perform inference and evaluation, you can find your predictions in `${nnUNet_raw_data_base}/nnUNet_raw_data/Task507_KiTS_kidney/predict/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ecdace",
      "metadata": {},
      "outputs": [],
      "source": [
        "! cd modelzoo/unet && sh scripts/run_predict.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5931f99",
      "metadata": {},
      "source": [
        "## 2.6. <a id='toc2_6_'></a>[Prediction Visualization](#toc0_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f7f11904",
      "metadata": {
        "id": "f7f11904"
      },
      "source": [
        "- For this we would advise to use [MITK](https://www.mitk.org/wiki/The_Medical_Imaging_Interaction_Toolkit_(MITK)) which already has some great [tutorials](https://www.mitk.org/wiki/Tutorials). \n",
        "    - If you have not already downloaded it, here is the [MITK Download Link](https://www.mitk.org/wiki/Downloads)\n",
        "- Here is a demostration of visualization result from MITK on KiTS dataset\n",
        "\n",
        "![KiTS_visualization](../imgs/KiTS_visualization.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e903885",
      "metadata": {
        "id": "8e903885"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
