2023-12-04 21:51:42.150460: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-04 21:51:42.150508: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-04 21:51:42.150551: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-04 21:51:42.887100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
finetune_args is 
 FinetuneArguments(lora_rank=8, lora_alpha=16, lora_dropout=0.05, lora_target_modules=None, adapter_layers=30, adapter_len=10, num_virtual_tokens=10, ptun_hidden_size=1024, peft='lora', resume_peft='', delta='lora', profile=False, train_on_inputs=True, habana=False, debugs=False, save_merged_model=False, merge_model_code_dir='', load_in_8bit=True, input_sentence='', output_length_limit=20)
2023-12-04 21:51:43,856 - __main__ - WARNING - Process rank: 0, device: cuda:0
distributed training: True, 16-bits training: True
2023-12-04 21:51:43,857 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora/runs/Dec04_21-51-43_vsr134,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'codecarbon'],
resume_from_checkpoint=None,
run_name=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:715] 2023-12-04 21:51:43,857 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:777] 2023-12-04 21:51:43,858 >> Model config LlamaConfig {
  "_name_or_path": "/home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:51:43,860 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:51:43,860 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:51:43,860 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:51:43,860 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:51:43,860 >> loading file tokenizer_config.json
/opt/conda/lib/python3.10/site-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:2735] 2023-12-04 21:51:45,626 >> The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' 
[INFO|modeling_utils.py:3118] 2023-12-04 21:51:45,626 >> loading weights file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1222] 2023-12-04 21:51:45,626 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:796] 2023-12-04 21:51:45,627 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

[INFO|modeling_utils.py:3255] 2023-12-04 21:51:45,828 >> Detected 8-bit loading: activating 8-bit loading for this model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.24s/it]
[INFO|modeling_utils.py:3950] 2023-12-04 21:51:52,506 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3958] 2023-12-04 21:51:52,507 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:749] 2023-12-04 21:51:52,509 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:796] 2023-12-04 21:51:52,510 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

2023-12-04 21:51:52,841 - __main__ - INFO - Using data collator of type DataCollatorForSeq2Seq
2023-12-04 21:52:00,084 - __main__ - INFO - ***original optimized model parameter***
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
2023-12-04 21:52:00,086 - root - INFO - The user specifies or automatically adjasts lora algo on model llama with enable denas
2023-12-04 21:52:03,059 - __main__ - INFO - ***deltatuner optimized model parameter***
trainable params: 2,375,680 || all params: 6,740,791,296 || trainable%: 0.035243340072103016
[INFO|trainer.py:593] 2023-12-04 21:52:03,164 >> Using auto half precision backend
/opt/conda/lib/python3.10/site-packages/tzlocal/unix.py:193: UserWarning: Can not find any timezone configuration, defaulting to UTC.
  warnings.warn("Can not find any timezone configuration, defaulting to UTC.")
2023-12-04 21:52:03,172 - apscheduler.scheduler - INFO - Adding job tentatively -- it will be properly scheduled when the scheduler starts
2023-12-04 21:52:03,172 - __main__ - INFO - *** Training ***
[INFO|trainer.py:1724] 2023-12-04 21:52:03,489 >> ***** Running training *****
[INFO|trainer.py:1725] 2023-12-04 21:52:03,489 >>   Num examples = 276
[INFO|trainer.py:1726] 2023-12-04 21:52:03,489 >>   Num Epochs = 1
[INFO|trainer.py:1727] 2023-12-04 21:52:03,489 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:1730] 2023-12-04 21:52:03,489 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1731] 2023-12-04 21:52:03,489 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1732] 2023-12-04 21:52:03,489 >>   Total optimization steps = 34
[INFO|trainer.py:1733] 2023-12-04 21:52:03,490 >>   Number of trainable parameters = 2,375,680
2023-12-04 21:52:03,494 - apscheduler.scheduler - INFO - Added job "EmissionsTracker._measure_power" to job store "default"
2023-12-04 21:52:03,494 - apscheduler.scheduler - INFO - Scheduler started
  0%|          | 0/34 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-12-04 21:52:03,500 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  3%|▎         | 1/34 [00:03<02:10,  3.95s/it]  6%|▌         | 2/34 [00:07<01:55,  3.60s/it]  9%|▉         | 3/34 [00:10<01:48,  3.48s/it] 12%|█▏        | 4/34 [00:14<01:43,  3.44s/it]2023-12-04 21:52:18,174 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:52:18 UTC)" (scheduled at 2023-12-04 21:52:18.172304+00:00)
2023-12-04 21:52:18,193 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:52:33 UTC)" executed successfully
 15%|█▍        | 5/34 [00:17<01:38,  3.41s/it] 18%|█▊        | 6/34 [00:20<01:34,  3.39s/it] 21%|██        | 7/34 [00:24<01:31,  3.38s/it] 24%|██▎       | 8/34 [00:27<01:27,  3.37s/it]2023-12-04 21:52:33,174 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:52:48 UTC)" (scheduled at 2023-12-04 21:52:33.172304+00:00)
2023-12-04 21:52:33,191 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:52:48 UTC)" executed successfully
 26%|██▋       | 9/34 [00:30<01:24,  3.37s/it] 29%|██▉       | 10/34 [00:34<01:20,  3.37s/it] 32%|███▏      | 11/34 [00:37<01:17,  3.37s/it] 35%|███▌      | 12/34 [00:40<01:14,  3.37s/it] 38%|███▊      | 13/34 [00:44<01:10,  3.37s/it]2023-12-04 21:52:48,174 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:53:03 UTC)" (scheduled at 2023-12-04 21:52:48.172304+00:00)
2023-12-04 21:52:48,193 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:53:03 UTC)" executed successfully
 41%|████      | 14/34 [00:47<01:07,  3.37s/it] 44%|████▍     | 15/34 [00:51<01:04,  3.38s/it] 47%|████▋     | 16/34 [00:54<01:01,  3.42s/it] 50%|█████     | 17/34 [00:58<00:59,  3.52s/it]2023-12-04 21:53:03,175 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:53:18 UTC)" (scheduled at 2023-12-04 21:53:03.172304+00:00)
2023-12-04 21:53:03,193 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:53:18 UTC)" executed successfully
 53%|█████▎    | 18/34 [01:02<00:58,  3.66s/it] 56%|█████▌    | 19/34 [01:06<00:57,  3.84s/it] 59%|█████▉    | 20/34 [01:11<00:57,  4.12s/it]2023-12-04 21:53:18,174 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:53:33 UTC)" (scheduled at 2023-12-04 21:53:18.172304+00:00)
2023-12-04 21:53:18,211 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:53:33 UTC)" executed successfully
 62%|██████▏   | 21/34 [01:16<00:59,  4.57s/it] 65%|██████▍   | 22/34 [01:23<01:02,  5.17s/it]2023-12-04 21:53:33,178 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:53:48 UTC)" (scheduled at 2023-12-04 21:53:33.172304+00:00)
2023-12-04 21:53:33,223 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:53:48 UTC)" executed successfully
 68%|██████▊   | 23/34 [01:30<01:04,  5.84s/it] 71%|███████   | 24/34 [01:39<01:05,  6.59s/it]2023-12-04 21:53:48,179 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:54:03 UTC)" (scheduled at 2023-12-04 21:53:48.172304+00:00)
2023-12-04 21:53:48,204 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:54:03 UTC)" executed successfully
 74%|███████▎  | 25/34 [01:50<01:11,  7.91s/it]2023-12-04 21:54:03,173 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:54:18 UTC)" (scheduled at 2023-12-04 21:54:03.172304+00:00)
2023-12-04 21:54:03,204 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:54:18 UTC)" executed successfully
 76%|███████▋  | 26/34 [02:00<01:08,  8.53s/it] 79%|███████▉  | 27/34 [02:12<01:06,  9.53s/it]2023-12-04 21:54:18,174 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:54:33 UTC)" (scheduled at 2023-12-04 21:54:18.172304+00:00)
2023-12-04 21:54:18,191 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:54:33 UTC)" executed successfully
 82%|████████▏ | 28/34 [02:23<01:01, 10.22s/it]2023-12-04 21:54:33,173 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:54:48 UTC)" (scheduled at 2023-12-04 21:54:33.172304+00:00)
2023-12-04 21:54:33,200 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:54:48 UTC)" executed successfully
 85%|████████▌ | 29/34 [02:37<00:55, 11.10s/it]2023-12-04 21:54:48,173 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:55:03 UTC)" (scheduled at 2023-12-04 21:54:48.172304+00:00)
2023-12-04 21:54:48,213 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:55:03 UTC)" executed successfully
 88%|████████▊ | 30/34 [02:49<00:45, 11.46s/it]2023-12-04 21:55:03,181 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:55:18 UTC)" (scheduled at 2023-12-04 21:55:03.172304+00:00)
2023-12-04 21:55:03,210 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:55:18 UTC)" executed successfully
 91%|█████████ | 31/34 [03:02<00:35, 11.88s/it]2023-12-04 21:55:18,174 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:55:33 UTC)" (scheduled at 2023-12-04 21:55:18.172304+00:00)
2023-12-04 21:55:18,214 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:55:33 UTC)" executed successfully
 94%|█████████▍| 32/34 [03:14<00:24, 12.11s/it] 97%|█████████▋| 33/34 [03:29<00:12, 12.89s/it]2023-12-04 21:55:33,174 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:55:48 UTC)" (scheduled at 2023-12-04 21:55:33.172304+00:00)
2023-12-04 21:55:33,197 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 21:55:48 UTC)" executed successfully
100%|██████████| 34/34 [03:43<00:00, 13.24s/it][INFO|trainer.py:2896] 2023-12-04 21:55:47,173 >> Saving model checkpoint to /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora/checkpoint-34
[INFO|tokenization_utils_base.py:2434] 2023-12-04 21:55:47,199 >> tokenizer config file saved in /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora/checkpoint-34/tokenizer_config.json
[INFO|tokenization_utils_base.py:2443] 2023-12-04 21:55:47,200 >> Special tokens file saved in /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora/checkpoint-34/special_tokens_map.json
[INFO|trainer.py:1967] 2023-12-04 21:55:47,265 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 223.7744, 'train_samples_per_second': 1.233, 'train_steps_per_second': 0.152, 'train_loss': 1.5011154623592602, 'epoch': 0.99}
100%|██████████| 34/34 [03:43<00:00, 13.24s/it]2023-12-04 21:55:47,267 - apscheduler.scheduler - INFO - Scheduler has been shut down
100%|██████████| 34/34 [03:47<00:00,  6.69s/it]
2023-12-04 21:55:56.483737: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-04 21:55:56.483783: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-04 21:55:56.483811: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-04 21:55:57.203968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
finetune_args is 
 FinetuneArguments(lora_rank=8, lora_alpha=16, lora_dropout=0.05, lora_target_modules=None, adapter_layers=30, adapter_len=10, num_virtual_tokens=10, ptun_hidden_size=1024, peft='lora', resume_peft='/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora', delta='lora', profile=False, train_on_inputs=True, habana=False, debugs=False, save_merged_model=False, merge_model_code_dir='', load_in_8bit=False, input_sentence='', output_length_limit=20)
2023-12-04 21:55:58,170 - __main__ - WARNING - Process rank: 0, device: cuda:0
distributed training: True, 16-bits training: False
2023-12-04 21:55:58,170 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora/runs/Dec04_21-55-58_vsr134,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'codecarbon'],
resume_from_checkpoint=None,
run_name=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:715] 2023-12-04 21:55:58,171 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:777] 2023-12-04 21:55:58,172 >> Model config LlamaConfig {
  "_name_or_path": "/home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:55:58,173 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:55:58,173 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:55:58,173 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:55:58,173 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:55:58,173 >> loading file tokenizer_config.json
/opt/conda/lib/python3.10/site-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:3118] 2023-12-04 21:55:59,881 >> loading weights file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/model.safetensors.index.json
[INFO|configuration_utils.py:796] 2023-12-04 21:55:59,882 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it]
[INFO|modeling_utils.py:3950] 2023-12-04 21:56:02,874 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3958] 2023-12-04 21:56:02,874 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:749] 2023-12-04 21:56:02,877 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:796] 2023-12-04 21:56:02,877 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

Map:   0%|          | 0/400 [00:00<?, ? examples/s]Map: 100%|██████████| 400/400 [00:00<00:00, 7805.57 examples/s]
2023-12-04 21:56:03,191 - __main__ - INFO - Using data collator of type DataCollatorForSeq2Seq
2023-12-04 21:56:17,836 - __main__ - INFO - ***original optimized model parameter***
trainable params: 0 || all params: 6,742,609,920 || trainable%: 0.0
2023-12-04 21:56:20,807 - __main__ - INFO - ***deltatuner optimized model parameter***
trainable params: 0 || all params: 6,740,791,296 || trainable%: 0.0
/opt/conda/lib/python3.10/site-packages/tzlocal/unix.py:193: UserWarning: Can not find any timezone configuration, defaulting to UTC.
  warnings.warn("Can not find any timezone configuration, defaulting to UTC.")
2023-12-04 21:56:37,649 - apscheduler.scheduler - INFO - Adding job tentatively -- it will be properly scheduled when the scheduler starts
2023-12-04 21:56:37,649 - __main__ - INFO - *** Evaluate ***
[INFO|trainer.py:738] 2023-12-04 21:56:37,649 >> The following columns in the evaluation set don't have a corresponding argument in `DelatunerModelForCausalLM.forward` and have been ignored: prompt_sources, prompt_targets. If prompt_sources, prompt_targets are not expected by `DelatunerModelForCausalLM.forward`,  you can safely ignore this message.
[INFO|trainer.py:3173] 2023-12-04 21:56:37,658 >> ***** Running Evaluation *****
[INFO|trainer.py:3175] 2023-12-04 21:56:37,658 >>   Num examples = 400
[INFO|trainer.py:3178] 2023-12-04 21:56:37,658 >>   Batch size = 4
[WARNING|logging.py:314] 2023-12-04 21:56:37,662 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:24,  4.00it/s]  3%|▎         | 3/100 [00:01<00:37,  2.62it/s]  4%|▍         | 4/100 [00:01<00:45,  2.09it/s]  5%|▌         | 5/100 [00:02<00:50,  1.88it/s]  6%|▌         | 6/100 [00:03<00:56,  1.67it/s]  7%|▋         | 7/100 [00:03<00:57,  1.63it/s]  8%|▊         | 8/100 [00:04<00:53,  1.72it/s]  9%|▉         | 9/100 [00:04<00:51,  1.76it/s] 10%|█         | 10/100 [00:05<00:53,  1.70it/s] 11%|█         | 11/100 [00:06<00:53,  1.66it/s] 12%|█▏        | 12/100 [00:06<00:56,  1.57it/s] 13%|█▎        | 13/100 [00:07<00:56,  1.54it/s] 14%|█▍        | 14/100 [00:08<00:55,  1.56it/s] 15%|█▌        | 15/100 [00:08<00:53,  1.58it/s] 16%|█▌        | 16/100 [00:09<00:51,  1.62it/s] 17%|█▋        | 17/100 [00:09<00:49,  1.68it/s] 18%|█▊        | 18/100 [00:10<00:47,  1.73it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:46,  1.72it/s] 21%|██        | 21/100 [00:12<00:46,  1.68it/s] 22%|██▏       | 22/100 [00:12<00:45,  1.71it/s] 23%|██▎       | 23/100 [00:13<00:45,  1.70it/s] 24%|██▍       | 24/100 [00:13<00:44,  1.69it/s] 25%|██▌       | 25/100 [00:14<00:44,  1.67it/s] 26%|██▌       | 26/100 [00:15<00:46,  1.61it/s] 27%|██▋       | 27/100 [00:15<00:47,  1.53it/s] 28%|██▊       | 28/100 [00:16<00:47,  1.51it/s] 29%|██▉       | 29/100 [00:17<00:46,  1.54it/s] 30%|███       | 30/100 [00:17<00:44,  1.56it/s] 31%|███       | 31/100 [00:18<00:44,  1.56it/s] 32%|███▏      | 32/100 [00:19<00:45,  1.49it/s] 33%|███▎      | 33/100 [00:19<00:44,  1.52it/s] 34%|███▍      | 34/100 [00:20<00:41,  1.57it/s] 35%|███▌      | 35/100 [00:20<00:40,  1.61it/s] 36%|███▌      | 36/100 [00:21<00:38,  1.65it/s] 37%|███▋      | 37/100 [00:22<00:37,  1.66it/s] 38%|███▊      | 38/100 [00:22<00:37,  1.63it/s] 39%|███▉      | 39/100 [00:23<00:37,  1.61it/s] 40%|████      | 40/100 [00:24<00:36,  1.63it/s] 41%|████      | 41/100 [00:24<00:35,  1.65it/s] 42%|████▏     | 42/100 [00:25<00:36,  1.60it/s] 43%|████▎     | 43/100 [00:26<00:37,  1.52it/s] 44%|████▍     | 44/100 [00:26<00:36,  1.52it/s] 45%|████▌     | 45/100 [00:27<00:37,  1.47it/s] 46%|████▌     | 46/100 [00:28<00:37,  1.44it/s] 47%|████▋     | 47/100 [00:28<00:37,  1.43it/s] 48%|████▊     | 48/100 [00:29<00:37,  1.40it/s] 49%|████▉     | 49/100 [00:30<00:37,  1.35it/s] 50%|█████     | 50/100 [00:31<00:37,  1.34it/s] 51%|█████     | 51/100 [00:31<00:35,  1.38it/s] 52%|█████▏    | 52/100 [00:32<00:34,  1.41it/s] 53%|█████▎    | 53/100 [00:33<00:32,  1.46it/s] 54%|█████▍    | 54/100 [00:33<00:30,  1.51it/s] 55%|█████▌    | 55/100 [00:34<00:29,  1.53it/s] 56%|█████▌    | 56/100 [00:35<00:28,  1.55it/s] 57%|█████▋    | 57/100 [00:35<00:27,  1.57it/s] 58%|█████▊    | 58/100 [00:36<00:26,  1.58it/s] 59%|█████▉    | 59/100 [00:36<00:25,  1.58it/s] 60%|██████    | 60/100 [00:37<00:25,  1.59it/s] 61%|██████    | 61/100 [00:38<00:24,  1.59it/s] 62%|██████▏   | 62/100 [00:38<00:24,  1.57it/s] 63%|██████▎   | 63/100 [00:39<00:23,  1.59it/s] 64%|██████▍   | 64/100 [00:40<00:23,  1.50it/s] 65%|██████▌   | 65/100 [00:40<00:24,  1.42it/s] 66%|██████▌   | 66/100 [00:41<00:24,  1.41it/s] 67%|██████▋   | 67/100 [00:42<00:23,  1.39it/s] 68%|██████▊   | 68/100 [00:43<00:22,  1.43it/s] 69%|██████▉   | 69/100 [00:43<00:21,  1.43it/s] 70%|███████   | 70/100 [00:44<00:21,  1.37it/s] 71%|███████   | 71/100 [00:45<00:21,  1.33it/s] 72%|███████▏  | 72/100 [00:46<00:20,  1.34it/s] 73%|███████▎  | 73/100 [00:46<00:21,  1.28it/s] 74%|███████▍  | 74/100 [00:47<00:20,  1.28it/s] 75%|███████▌  | 75/100 [00:48<00:18,  1.34it/s] 76%|███████▌  | 76/100 [00:48<00:16,  1.44it/s] 77%|███████▋  | 77/100 [00:49<00:15,  1.46it/s] 78%|███████▊  | 78/100 [00:50<00:15,  1.45it/s] 79%|███████▉  | 79/100 [00:51<00:15,  1.40it/s] 80%|████████  | 80/100 [00:51<00:15,  1.31it/s] 81%|████████  | 81/100 [00:52<00:14,  1.31it/s] 82%|████████▏ | 82/100 [00:53<00:13,  1.36it/s] 83%|████████▎ | 83/100 [00:54<00:12,  1.35it/s] 84%|████████▍ | 84/100 [00:55<00:12,  1.30it/s] 85%|████████▌ | 85/100 [00:55<00:11,  1.28it/s] 86%|████████▌ | 86/100 [00:56<00:11,  1.25it/s] 87%|████████▋ | 87/100 [00:57<00:10,  1.24it/s] 88%|████████▊ | 88/100 [00:58<00:09,  1.30it/s] 89%|████████▉ | 89/100 [00:58<00:08,  1.29it/s] 90%|█████████ | 90/100 [00:59<00:08,  1.23it/s] 91%|█████████ | 91/100 [01:00<00:07,  1.26it/s] 92%|█████████▏| 92/100 [01:01<00:05,  1.37it/s] 93%|█████████▎| 93/100 [01:01<00:05,  1.34it/s] 94%|█████████▍| 94/100 [01:02<00:04,  1.29it/s] 95%|█████████▌| 95/100 [01:03<00:03,  1.33it/s] 96%|█████████▌| 96/100 [01:04<00:02,  1.35it/s] 97%|█████████▋| 97/100 [01:04<00:02,  1.34it/s] 98%|█████████▊| 98/100 [01:05<00:01,  1.37it/s] 99%|█████████▉| 99/100 [01:06<00:00,  1.36it/s]100%|██████████| 100/100 [01:07<00:00,  1.35it/s]100%|██████████| 100/100 [01:07<00:00,  1.48it/s]
Map:   0%|          | 0/400 [00:00<?, ? examples/s]Map: 100%|██████████| 400/400 [00:00<00:00, 8977.73 examples/s]
***** eval metrics *****
  eval_loss               =     2.0169
  eval_runtime            = 0:01:08.23
  eval_samples            =        400
  eval_samples_per_second =      5.862
  eval_steps_per_second   =      1.465
  eval_tokens             =      39160
2023-12-04 21:57:51.284669: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-04 21:57:51.284718: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-04 21:57:51.284744: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-04 21:57:52.003154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
finetune_args is 
 FinetuneArguments(lora_rank=8, lora_alpha=16, lora_dropout=0.05, lora_target_modules=None, adapter_layers=30, adapter_len=10, num_virtual_tokens=10, ptun_hidden_size=1024, peft='lora', resume_peft='/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora', delta='lora', profile=False, train_on_inputs=True, habana=False, debugs=False, save_merged_model=True, merge_model_code_dir='', load_in_8bit=False, input_sentence='', output_length_limit=20)
2023-12-04 21:57:52,974 - __main__ - WARNING - Process rank: 0, device: cuda:0
distributed training: True, 16-bits training: True
2023-12-04 21:57:52,974 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora/runs/Dec04_21-57-52_vsr134,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'codecarbon'],
resume_from_checkpoint=None,
run_name=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:715] 2023-12-04 21:57:52,975 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:777] 2023-12-04 21:57:52,976 >> Model config LlamaConfig {
  "_name_or_path": "/home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:57:52,977 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:57:52,977 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:57:52,977 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:57:52,977 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 21:57:52,977 >> loading file tokenizer_config.json
/opt/conda/lib/python3.10/site-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:3118] 2023-12-04 21:57:54,695 >> loading weights file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1222] 2023-12-04 21:57:54,696 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:796] 2023-12-04 21:57:54,696 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.49it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.78it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.73it/s]
[INFO|modeling_utils.py:3950] 2023-12-04 21:57:55,343 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3958] 2023-12-04 21:57:55,343 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:749] 2023-12-04 21:57:55,345 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:796] 2023-12-04 21:57:55,346 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

Map:   0%|          | 0/1423 [00:00<?, ? examples/s]Map:  70%|███████   | 1000/1423 [00:00<00:00, 7870.66 examples/s]Map: 100%|██████████| 1423/1423 [00:00<00:00, 7706.73 examples/s]
2023-12-04 21:57:55,756 - __main__ - INFO - Using data collator of type DataCollatorForSeq2Seq
2023-12-04 21:58:10,105 - __main__ - INFO - ***original optimized model parameter***
trainable params: 0 || all params: 6,742,609,920 || trainable%: 0.0
2023-12-04 21:58:13,098 - __main__ - INFO - ***deltatuner optimized model parameter***
trainable params: 0 || all params: 6,740,791,296 || trainable%: 0.0
[INFO|trainer.py:593] 2023-12-04 21:58:17,131 >> Using auto half precision backend
/opt/conda/lib/python3.10/site-packages/tzlocal/unix.py:193: UserWarning: Can not find any timezone configuration, defaulting to UTC.
  warnings.warn("Can not find any timezone configuration, defaulting to UTC.")
2023-12-04 21:58:17,137 - apscheduler.scheduler - INFO - Adding job tentatively -- it will be properly scheduled when the scheduler starts
copy base model config to /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora/merged_model
cp: -r not specified; omitting directory '/home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/analysis'
remove unnecessary file from /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf
Save merged model to /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa/models/Llama-2-7b-hf_denas-lora/merged_model
2023-12-04 22:00:55.196308: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-04 22:00:55.196357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-04 22:00:55.196382: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-04 22:00:55.919489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
finetune_args is 
 FinetuneArguments(lora_rank=8, lora_alpha=16, lora_dropout=0.05, lora_target_modules=None, adapter_layers=30, adapter_len=10, num_virtual_tokens=10, ptun_hidden_size=1024, peft='lora', resume_peft='', delta='lora', profile=False, train_on_inputs=True, habana=False, debugs=False, save_merged_model=False, merge_model_code_dir='', load_in_8bit=True, input_sentence='', output_length_limit=20)
2023-12-04 22:00:56,878 - __main__ - WARNING - Process rank: 0, device: cuda:0
distributed training: True, 16-bits training: True
2023-12-04 22:00:56,879 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora/runs/Dec04_22-00-56_vsr134,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'codecarbon'],
resume_from_checkpoint=None,
run_name=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:715] 2023-12-04 22:00:56,879 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:777] 2023-12-04 22:00:56,880 >> Model config LlamaConfig {
  "_name_or_path": "/home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:00:56,882 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:00:56,882 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:00:56,882 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:00:56,882 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:00:56,882 >> loading file tokenizer_config.json
/opt/conda/lib/python3.10/site-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:2735] 2023-12-04 22:00:58,625 >> The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' 
[INFO|modeling_utils.py:3118] 2023-12-04 22:00:58,625 >> loading weights file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1222] 2023-12-04 22:00:58,625 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:796] 2023-12-04 22:00:58,626 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

[INFO|modeling_utils.py:3255] 2023-12-04 22:00:58,813 >> Detected 8-bit loading: activating 8-bit loading for this model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]
[INFO|modeling_utils.py:3950] 2023-12-04 22:01:05,558 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3958] 2023-12-04 22:01:05,558 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:749] 2023-12-04 22:01:05,561 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:796] 2023-12-04 22:01:05,561 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

Map:   0%|          | 0/1423 [00:00<?, ? examples/s]Map:  70%|███████   | 1000/1423 [00:00<00:00, 8678.94 examples/s]Map: 100%|██████████| 1423/1423 [00:00<00:00, 8907.06 examples/s]
2023-12-04 22:01:05,855 - __main__ - INFO - Using data collator of type DataCollatorForSeq2Seq
2023-12-04 22:01:13,120 - __main__ - INFO - ***original optimized model parameter***
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
2023-12-04 22:01:13,123 - root - INFO - The user specifies or automatically adjasts lora algo on model llama with enable denas
2023-12-04 22:01:16,101 - __main__ - INFO - ***deltatuner optimized model parameter***
trainable params: 2,375,680 || all params: 6,740,791,296 || trainable%: 0.035243340072103016
[INFO|trainer.py:593] 2023-12-04 22:01:16,179 >> Using auto half precision backend
/opt/conda/lib/python3.10/site-packages/tzlocal/unix.py:193: UserWarning: Can not find any timezone configuration, defaulting to UTC.
  warnings.warn("Can not find any timezone configuration, defaulting to UTC.")
2023-12-04 22:01:16,185 - apscheduler.scheduler - INFO - Adding job tentatively -- it will be properly scheduled when the scheduler starts
2023-12-04 22:01:16,185 - __main__ - INFO - *** Training ***
[INFO|trainer.py:1724] 2023-12-04 22:01:16,495 >> ***** Running training *****
[INFO|trainer.py:1725] 2023-12-04 22:01:16,495 >>   Num examples = 1,423
[INFO|trainer.py:1726] 2023-12-04 22:01:16,495 >>   Num Epochs = 1
[INFO|trainer.py:1727] 2023-12-04 22:01:16,495 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:1730] 2023-12-04 22:01:16,495 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1731] 2023-12-04 22:01:16,495 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1732] 2023-12-04 22:01:16,495 >>   Total optimization steps = 178
[INFO|trainer.py:1733] 2023-12-04 22:01:16,496 >>   Number of trainable parameters = 2,375,680
2023-12-04 22:01:16,500 - apscheduler.scheduler - INFO - Added job "EmissionsTracker._measure_power" to job store "default"
2023-12-04 22:01:16,500 - apscheduler.scheduler - INFO - Scheduler started
  0%|          | 0/178 [00:00<?, ?it/s][WARNING|logging.py:314] 2023-12-04 22:01:16,505 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  1%|          | 1/178 [00:02<07:11,  2.44s/it]  1%|          | 2/178 [00:04<06:13,  2.12s/it]  2%|▏         | 3/178 [00:06<05:42,  1.96s/it]  2%|▏         | 4/178 [00:07<05:27,  1.88s/it]  3%|▎         | 5/178 [00:09<05:16,  1.83s/it]  3%|▎         | 6/178 [00:11<05:09,  1.80s/it]  4%|▍         | 7/178 [00:13<05:05,  1.79s/it]2023-12-04 22:01:31,188 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:01:31 UTC)" (scheduled at 2023-12-04 22:01:31.185772+00:00)
2023-12-04 22:01:31,208 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:01:46 UTC)" executed successfully
  4%|▍         | 8/178 [00:14<05:03,  1.79s/it]  5%|▌         | 9/178 [00:16<04:57,  1.76s/it]  6%|▌         | 10/178 [00:18<04:53,  1.75s/it]  6%|▌         | 11/178 [00:20<04:53,  1.76s/it]  7%|▋         | 12/178 [00:21<04:48,  1.74s/it]  7%|▋         | 13/178 [00:23<04:45,  1.73s/it]  8%|▊         | 14/178 [00:25<04:43,  1.73s/it]  8%|▊         | 15/178 [00:26<04:42,  1.73s/it]  9%|▉         | 16/178 [00:28<04:43,  1.75s/it]2023-12-04 22:01:46,188 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:02:01 UTC)" (scheduled at 2023-12-04 22:01:46.185772+00:00)
2023-12-04 22:01:46,205 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:02:01 UTC)" executed successfully
 10%|▉         | 17/178 [00:30<04:45,  1.77s/it] 10%|█         | 18/178 [00:32<04:48,  1.80s/it] 11%|█         | 19/178 [00:34<04:48,  1.81s/it] 11%|█         | 20/178 [00:36<04:47,  1.82s/it] 12%|█▏        | 21/178 [00:37<04:47,  1.83s/it] 12%|█▏        | 22/178 [00:39<04:48,  1.85s/it] 13%|█▎        | 23/178 [00:41<04:53,  1.89s/it] 13%|█▎        | 24/178 [00:44<05:03,  1.97s/it]2023-12-04 22:02:01,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:02:16 UTC)" (scheduled at 2023-12-04 22:02:01.185772+00:00)
2023-12-04 22:02:01,212 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:02:16 UTC)" executed successfully
 14%|█▍        | 25/178 [00:46<05:08,  2.02s/it] 15%|█▍        | 26/178 [00:48<05:17,  2.09s/it] 15%|█▌        | 27/178 [00:51<05:47,  2.30s/it] 16%|█▌        | 28/178 [00:54<06:08,  2.46s/it] 16%|█▋        | 29/178 [00:57<06:40,  2.69s/it]2023-12-04 22:02:16,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:02:31 UTC)" (scheduled at 2023-12-04 22:02:16.185772+00:00)
2023-12-04 22:02:16,220 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:02:31 UTC)" executed successfully
 17%|█▋        | 30/178 [01:00<06:51,  2.78s/it] 17%|█▋        | 31/178 [01:03<06:54,  2.82s/it] 18%|█▊        | 32/178 [01:05<06:52,  2.83s/it] 19%|█▊        | 33/178 [01:08<06:51,  2.84s/it] 19%|█▉        | 34/178 [01:11<06:49,  2.85s/it]2023-12-04 22:02:31,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:02:46 UTC)" (scheduled at 2023-12-04 22:02:31.185772+00:00)
2023-12-04 22:02:31,216 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:02:46 UTC)" executed successfully
 20%|█▉        | 35/178 [01:15<07:12,  3.02s/it] 20%|██        | 36/178 [01:19<07:51,  3.32s/it] 21%|██        | 37/178 [01:24<08:52,  3.77s/it] 21%|██▏       | 38/178 [01:27<08:22,  3.59s/it]2023-12-04 22:02:46,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:03:01 UTC)" (scheduled at 2023-12-04 22:02:46.185772+00:00)
2023-12-04 22:02:46,233 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:03:01 UTC)" executed successfully
 22%|██▏       | 39/178 [01:30<08:13,  3.55s/it] 22%|██▏       | 40/178 [01:34<08:21,  3.64s/it] 23%|██▎       | 41/178 [01:38<08:33,  3.75s/it] 24%|██▎       | 42/178 [01:43<09:23,  4.14s/it]2023-12-04 22:03:01,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:03:16 UTC)" (scheduled at 2023-12-04 22:03:01.185772+00:00)
2023-12-04 22:03:01,227 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:03:16 UTC)" executed successfully
 24%|██▍       | 43/178 [01:48<09:37,  4.28s/it] 25%|██▍       | 44/178 [01:52<09:46,  4.38s/it] 25%|██▌       | 45/178 [01:56<09:11,  4.14s/it]2023-12-04 22:03:16,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:03:31 UTC)" (scheduled at 2023-12-04 22:03:16.185772+00:00)
2023-12-04 22:03:16,203 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:03:31 UTC)" executed successfully
 26%|██▌       | 46/178 [02:02<10:19,  4.70s/it] 26%|██▋       | 47/178 [02:07<10:17,  4.71s/it] 27%|██▋       | 48/178 [02:11<10:13,  4.72s/it]2023-12-04 22:03:31,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:03:46 UTC)" (scheduled at 2023-12-04 22:03:31.185772+00:00)
2023-12-04 22:03:31,229 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:03:46 UTC)" executed successfully
 28%|██▊       | 49/178 [02:17<10:29,  4.88s/it] 28%|██▊       | 50/178 [02:21<09:54,  4.64s/it] 29%|██▊       | 51/178 [02:25<09:34,  4.52s/it]2023-12-04 22:03:46,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:04:01 UTC)" (scheduled at 2023-12-04 22:03:46.185772+00:00)
2023-12-04 22:03:46,217 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:04:01 UTC)" executed successfully
 29%|██▉       | 52/178 [02:31<10:36,  5.05s/it] 30%|██▉       | 53/178 [02:36<10:32,  5.06s/it] 30%|███       | 54/178 [02:40<09:37,  4.66s/it]2023-12-04 22:04:01,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:04:16 UTC)" (scheduled at 2023-12-04 22:04:01.185772+00:00)
2023-12-04 22:04:01,209 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:04:16 UTC)" executed successfully
 31%|███       | 55/178 [02:46<10:29,  5.12s/it] 31%|███▏      | 56/178 [02:50<09:43,  4.79s/it] 32%|███▏      | 57/178 [02:55<09:28,  4.70s/it]2023-12-04 22:04:16,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:04:31 UTC)" (scheduled at 2023-12-04 22:04:16.185772+00:00)
2023-12-04 22:04:16,216 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:04:31 UTC)" executed successfully
 33%|███▎      | 58/178 [03:01<10:18,  5.15s/it] 33%|███▎      | 59/178 [03:06<10:05,  5.09s/it] 34%|███▎      | 60/178 [03:10<09:14,  4.70s/it]2023-12-04 22:04:31,189 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:04:46 UTC)" (scheduled at 2023-12-04 22:04:31.185772+00:00)
2023-12-04 22:04:31,212 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:04:46 UTC)" executed successfully
 34%|███▍      | 61/178 [03:16<10:07,  5.19s/it] 35%|███▍      | 62/178 [03:20<09:14,  4.78s/it] 35%|███▌      | 63/178 [03:26<10:12,  5.33s/it]2023-12-04 22:04:46,190 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:05:01 UTC)" (scheduled at 2023-12-04 22:04:46.185772+00:00)
2023-12-04 22:04:46,213 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:05:01 UTC)" executed successfully
 36%|███▌      | 64/178 [03:32<10:13,  5.38s/it] 37%|███▋      | 65/178 [03:36<09:15,  4.92s/it] 37%|███▋      | 66/178 [03:40<08:43,  4.67s/it]2023-12-04 22:05:01,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:05:16 UTC)" (scheduled at 2023-12-04 22:05:01.185772+00:00)
2023-12-04 22:05:01,205 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:05:16 UTC)" executed successfully
 38%|███▊      | 67/178 [03:47<10:11,  5.51s/it] 38%|███▊      | 68/178 [03:52<09:24,  5.13s/it] 39%|███▉      | 69/178 [03:58<09:53,  5.45s/it]2023-12-04 22:05:16,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:05:31 UTC)" (scheduled at 2023-12-04 22:05:16.185772+00:00)
2023-12-04 22:05:16,226 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:05:31 UTC)" executed successfully
 39%|███▉      | 70/178 [04:03<09:44,  5.41s/it] 40%|███▉      | 71/178 [04:08<09:22,  5.26s/it]2023-12-04 22:05:31,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:05:46 UTC)" (scheduled at 2023-12-04 22:05:31.185772+00:00)
2023-12-04 22:05:31,212 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:05:46 UTC)" executed successfully
 40%|████      | 72/178 [04:15<10:19,  5.84s/it] 41%|████      | 73/178 [04:20<09:54,  5.66s/it] 42%|████▏     | 74/178 [04:25<09:31,  5.50s/it]2023-12-04 22:05:46,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:06:01 UTC)" (scheduled at 2023-12-04 22:05:46.185772+00:00)
2023-12-04 22:05:46,213 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:06:01 UTC)" executed successfully
 42%|████▏     | 75/178 [04:32<09:49,  5.72s/it] 43%|████▎     | 76/178 [04:35<08:40,  5.10s/it] 43%|████▎     | 77/178 [04:42<09:06,  5.41s/it]2023-12-04 22:06:01,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:06:16 UTC)" (scheduled at 2023-12-04 22:06:01.185772+00:00)
2023-12-04 22:06:01,227 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:06:16 UTC)" executed successfully
 44%|████▍     | 78/178 [04:47<08:59,  5.40s/it] 44%|████▍     | 79/178 [04:53<09:06,  5.52s/it] 45%|████▍     | 80/178 [04:58<08:58,  5.49s/it]2023-12-04 22:06:16,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:06:31 UTC)" (scheduled at 2023-12-04 22:06:16.185772+00:00)
2023-12-04 22:06:16,202 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:06:31 UTC)" executed successfully
 46%|████▌     | 81/178 [05:04<09:04,  5.61s/it] 46%|████▌     | 82/178 [05:09<08:48,  5.50s/it]2023-12-04 22:06:31,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:06:46 UTC)" (scheduled at 2023-12-04 22:06:31.185772+00:00)
2023-12-04 22:06:31,213 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:06:46 UTC)" executed successfully
 47%|████▋     | 83/178 [05:14<08:28,  5.36s/it] 47%|████▋     | 84/178 [05:20<08:28,  5.41s/it] 48%|████▊     | 85/178 [05:25<08:15,  5.32s/it]2023-12-04 22:06:46,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:07:01 UTC)" (scheduled at 2023-12-04 22:06:46.185772+00:00)
2023-12-04 22:06:46,224 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:07:01 UTC)" executed successfully
 48%|████▊     | 86/178 [05:31<08:37,  5.63s/it] 49%|████▉     | 87/178 [05:38<08:56,  5.90s/it] 49%|████▉     | 88/178 [05:43<08:32,  5.70s/it]2023-12-04 22:07:01,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:07:16 UTC)" (scheduled at 2023-12-04 22:07:01.185772+00:00)
2023-12-04 22:07:01,201 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:07:16 UTC)" executed successfully
 50%|█████     | 89/178 [05:48<08:16,  5.58s/it] 51%|█████     | 90/178 [05:53<07:55,  5.40s/it]2023-12-04 22:07:16,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:07:31 UTC)" (scheduled at 2023-12-04 22:07:16.185772+00:00)
2023-12-04 22:07:16,225 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:07:31 UTC)" executed successfully
 51%|█████     | 91/178 [05:59<08:08,  5.62s/it] 52%|█████▏    | 92/178 [06:02<06:41,  4.67s/it] 52%|█████▏    | 93/178 [06:05<05:49,  4.12s/it] 53%|█████▎    | 94/178 [06:08<05:17,  3.78s/it] 53%|█████▎    | 95/178 [06:10<04:46,  3.46s/it] 54%|█████▍    | 96/178 [06:13<04:17,  3.14s/it]2023-12-04 22:07:31,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:07:46 UTC)" (scheduled at 2023-12-04 22:07:31.185772+00:00)
2023-12-04 22:07:31,217 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:07:46 UTC)" executed successfully
 54%|█████▍    | 97/178 [06:16<04:03,  3.01s/it] 55%|█████▌    | 98/178 [06:18<03:48,  2.85s/it] 56%|█████▌    | 99/178 [06:20<03:28,  2.64s/it] 56%|█████▌    | 100/178 [06:22<03:14,  2.49s/it]                                                 {'loss': 1.2628, 'learning_rate': 4.3820224719101126e-05, 'epoch': 0.56}
 56%|█████▌    | 100/178 [06:22<03:14,  2.49s/it] 57%|█████▋    | 101/178 [06:24<03:00,  2.35s/it] 57%|█████▋    | 102/178 [06:26<02:52,  2.27s/it] 58%|█████▊    | 103/178 [06:28<02:42,  2.17s/it]2023-12-04 22:07:46,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:08:01 UTC)" (scheduled at 2023-12-04 22:07:46.185772+00:00)
2023-12-04 22:07:46,211 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:08:01 UTC)" executed successfully
 58%|█████▊    | 104/178 [06:30<02:36,  2.11s/it] 59%|█████▉    | 105/178 [06:32<02:30,  2.06s/it] 60%|█████▉    | 106/178 [06:34<02:24,  2.01s/it] 60%|██████    | 107/178 [06:36<02:23,  2.02s/it] 61%|██████    | 108/178 [06:38<02:18,  1.98s/it] 61%|██████    | 109/178 [06:40<02:16,  1.98s/it] 62%|██████▏   | 110/178 [06:42<02:15,  1.99s/it] 62%|██████▏   | 111/178 [06:44<02:13,  1.99s/it]2023-12-04 22:08:01,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:08:16 UTC)" (scheduled at 2023-12-04 22:08:01.185772+00:00)
2023-12-04 22:08:01,202 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:08:16 UTC)" executed successfully
 63%|██████▎   | 112/178 [06:46<02:12,  2.01s/it] 63%|██████▎   | 113/178 [06:48<02:08,  1.98s/it] 64%|██████▍   | 114/178 [06:50<02:04,  1.94s/it] 65%|██████▍   | 115/178 [06:52<02:01,  1.92s/it] 65%|██████▌   | 116/178 [06:54<01:57,  1.90s/it] 66%|██████▌   | 117/178 [06:55<01:54,  1.88s/it] 66%|██████▋   | 118/178 [06:57<01:53,  1.89s/it] 67%|██████▋   | 119/178 [06:59<01:50,  1.87s/it]2023-12-04 22:08:16,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:08:31 UTC)" (scheduled at 2023-12-04 22:08:16.185772+00:00)
2023-12-04 22:08:16,205 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:08:31 UTC)" executed successfully
 67%|██████▋   | 120/178 [07:01<01:49,  1.88s/it] 68%|██████▊   | 121/178 [07:03<01:48,  1.90s/it] 69%|██████▊   | 122/178 [07:05<01:45,  1.89s/it] 69%|██████▉   | 123/178 [07:07<01:44,  1.90s/it] 70%|██████▉   | 124/178 [07:09<01:42,  1.91s/it] 70%|███████   | 125/178 [07:11<01:40,  1.89s/it] 71%|███████   | 126/178 [07:12<01:37,  1.88s/it]2023-12-04 22:08:31,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:08:46 UTC)" (scheduled at 2023-12-04 22:08:31.185772+00:00)
2023-12-04 22:08:31,203 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:08:46 UTC)" executed successfully
 71%|███████▏  | 127/178 [07:14<01:35,  1.87s/it] 72%|███████▏  | 128/178 [07:16<01:32,  1.86s/it] 72%|███████▏  | 129/178 [07:18<01:31,  1.86s/it] 73%|███████▎  | 130/178 [07:20<01:28,  1.85s/it] 74%|███████▎  | 131/178 [07:22<01:27,  1.86s/it] 74%|███████▍  | 132/178 [07:24<01:24,  1.85s/it] 75%|███████▍  | 133/178 [07:25<01:23,  1.84s/it] 75%|███████▌  | 134/178 [07:27<01:23,  1.91s/it]2023-12-04 22:08:46,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:09:01 UTC)" (scheduled at 2023-12-04 22:08:46.185772+00:00)
2023-12-04 22:08:46,204 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:09:01 UTC)" executed successfully
 76%|███████▌  | 135/178 [07:29<01:20,  1.88s/it] 76%|███████▋  | 136/178 [07:31<01:18,  1.88s/it] 77%|███████▋  | 137/178 [07:33<01:16,  1.86s/it] 78%|███████▊  | 138/178 [07:35<01:15,  1.88s/it] 78%|███████▊  | 139/178 [07:37<01:13,  1.89s/it] 79%|███████▊  | 140/178 [07:39<01:12,  1.91s/it] 79%|███████▉  | 141/178 [07:41<01:10,  1.89s/it] 80%|███████▉  | 142/178 [07:42<01:07,  1.88s/it]2023-12-04 22:09:01,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:09:16 UTC)" (scheduled at 2023-12-04 22:09:01.185772+00:00)
2023-12-04 22:09:01,204 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:09:16 UTC)" executed successfully
 80%|████████  | 143/178 [07:44<01:05,  1.86s/it] 81%|████████  | 144/178 [07:46<01:05,  1.93s/it] 81%|████████▏ | 145/178 [07:48<01:03,  1.93s/it] 82%|████████▏ | 146/178 [07:50<01:01,  1.92s/it] 83%|████████▎ | 147/178 [07:52<00:58,  1.88s/it] 83%|████████▎ | 148/178 [07:54<00:55,  1.85s/it] 84%|████████▎ | 149/178 [07:55<00:53,  1.83s/it] 84%|████████▍ | 150/178 [07:57<00:51,  1.84s/it]2023-12-04 22:09:16,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:09:31 UTC)" (scheduled at 2023-12-04 22:09:16.185772+00:00)
2023-12-04 22:09:16,201 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:09:31 UTC)" executed successfully
 85%|████████▍ | 151/178 [07:59<00:49,  1.84s/it] 85%|████████▌ | 152/178 [08:01<00:47,  1.83s/it] 86%|████████▌ | 153/178 [08:03<00:45,  1.83s/it] 87%|████████▋ | 154/178 [08:05<00:44,  1.87s/it] 87%|████████▋ | 155/178 [08:07<00:42,  1.86s/it] 88%|████████▊ | 156/178 [08:09<00:41,  1.89s/it] 88%|████████▊ | 157/178 [08:10<00:39,  1.87s/it] 89%|████████▉ | 158/178 [08:12<00:37,  1.85s/it] 89%|████████▉ | 159/178 [08:14<00:35,  1.85s/it]2023-12-04 22:09:31,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:09:46 UTC)" (scheduled at 2023-12-04 22:09:31.185772+00:00)
2023-12-04 22:09:31,211 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:09:46 UTC)" executed successfully
 90%|████████▉ | 160/178 [08:16<00:33,  1.87s/it] 90%|█████████ | 161/178 [08:18<00:31,  1.86s/it] 91%|█████████ | 162/178 [08:20<00:29,  1.84s/it] 92%|█████████▏| 163/178 [08:22<00:28,  1.87s/it] 92%|█████████▏| 164/178 [08:23<00:26,  1.86s/it] 93%|█████████▎| 165/178 [08:25<00:24,  1.87s/it] 93%|█████████▎| 166/178 [08:27<00:22,  1.85s/it] 94%|█████████▍| 167/178 [08:29<00:20,  1.83s/it]2023-12-04 22:09:46,187 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:10:01 UTC)" (scheduled at 2023-12-04 22:09:46.185772+00:00)
2023-12-04 22:09:46,203 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:10:01 UTC)" executed successfully
 94%|█████████▍| 168/178 [08:31<00:18,  1.83s/it] 95%|█████████▍| 169/178 [08:33<00:16,  1.84s/it] 96%|█████████▌| 170/178 [08:35<00:14,  1.87s/it] 96%|█████████▌| 171/178 [08:36<00:12,  1.85s/it] 97%|█████████▋| 172/178 [08:38<00:11,  1.86s/it] 97%|█████████▋| 173/178 [08:40<00:09,  1.84s/it] 98%|█████████▊| 174/178 [08:42<00:07,  1.83s/it] 98%|█████████▊| 175/178 [08:44<00:05,  1.84s/it]2023-12-04 22:10:01,186 - apscheduler.executors.default - INFO - Running job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:10:16 UTC)" (scheduled at 2023-12-04 22:10:01.185772+00:00)
2023-12-04 22:10:01,203 - apscheduler.executors.default - INFO - Job "EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-12-04 22:10:16 UTC)" executed successfully
 99%|█████████▉| 176/178 [08:46<00:03,  1.84s/it] 99%|█████████▉| 177/178 [08:47<00:01,  1.85s/it]100%|██████████| 178/178 [08:49<00:00,  1.83s/it][INFO|trainer.py:2896] 2023-12-04 22:10:06,188 >> Saving model checkpoint to /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora/checkpoint-178
[INFO|tokenization_utils_base.py:2434] 2023-12-04 22:10:06,214 >> tokenizer config file saved in /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora/checkpoint-178/tokenizer_config.json
[INFO|tokenization_utils_base.py:2443] 2023-12-04 22:10:06,214 >> Special tokens file saved in /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora/checkpoint-178/special_tokens_map.json
[INFO|trainer.py:1967] 2023-12-04 22:10:06,275 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 529.7788, 'train_samples_per_second': 2.686, 'train_steps_per_second': 0.336, 'train_loss': 1.127147717422314, 'epoch': 1.0}
100%|██████████| 178/178 [08:49<00:00,  1.83s/it]2023-12-04 22:10:06,277 - apscheduler.scheduler - INFO - Scheduler has been shut down
100%|██████████| 178/178 [08:53<00:00,  3.00s/it]
2023-12-04 22:10:15.702177: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-04 22:10:15.702227: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-04 22:10:15.702251: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-04 22:10:16.422249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
finetune_args is 
 FinetuneArguments(lora_rank=8, lora_alpha=16, lora_dropout=0.05, lora_target_modules=None, adapter_layers=30, adapter_len=10, num_virtual_tokens=10, ptun_hidden_size=1024, peft='lora', resume_peft='/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora', delta='lora', profile=False, train_on_inputs=True, habana=False, debugs=False, save_merged_model=False, merge_model_code_dir='', load_in_8bit=False, input_sentence='', output_length_limit=20)
2023-12-04 22:10:17,397 - __main__ - WARNING - Process rank: 0, device: cuda:0
distributed training: True, 16-bits training: False
2023-12-04 22:10:17,397 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora/runs/Dec04_22-10-17_vsr134,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'codecarbon'],
resume_from_checkpoint=None,
run_name=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:715] 2023-12-04 22:10:17,398 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:777] 2023-12-04 22:10:17,399 >> Model config LlamaConfig {
  "_name_or_path": "/home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:10:17,400 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:10:17,400 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:10:17,400 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:10:17,400 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:10:17,400 >> loading file tokenizer_config.json
/opt/conda/lib/python3.10/site-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:3118] 2023-12-04 22:10:19,184 >> loading weights file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/model.safetensors.index.json
[INFO|configuration_utils.py:796] 2023-12-04 22:10:19,185 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]
[INFO|modeling_utils.py:3950] 2023-12-04 22:10:22,158 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3958] 2023-12-04 22:10:22,158 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:749] 2023-12-04 22:10:22,161 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:796] 2023-12-04 22:10:22,161 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

Map:   0%|          | 0/1423 [00:00<?, ? examples/s]Map:  70%|███████   | 1000/1423 [00:00<00:00, 7920.43 examples/s]Map: 100%|██████████| 1423/1423 [00:00<00:00, 8102.89 examples/s]
Map:   0%|          | 0/400 [00:00<?, ? examples/s]Map: 100%|██████████| 400/400 [00:00<00:00, 6910.52 examples/s]
2023-12-04 22:10:22,466 - __main__ - INFO - Using data collator of type DataCollatorForSeq2Seq
2023-12-04 22:10:37,034 - __main__ - INFO - ***original optimized model parameter***
trainable params: 0 || all params: 6,742,609,920 || trainable%: 0.0
2023-12-04 22:10:40,097 - __main__ - INFO - ***deltatuner optimized model parameter***
trainable params: 0 || all params: 6,740,791,296 || trainable%: 0.0
/opt/conda/lib/python3.10/site-packages/tzlocal/unix.py:193: UserWarning: Can not find any timezone configuration, defaulting to UTC.
  warnings.warn("Can not find any timezone configuration, defaulting to UTC.")
2023-12-04 22:10:57,263 - apscheduler.scheduler - INFO - Adding job tentatively -- it will be properly scheduled when the scheduler starts
2023-12-04 22:10:57,263 - __main__ - INFO - *** Evaluate ***
[INFO|trainer.py:738] 2023-12-04 22:10:57,264 >> The following columns in the evaluation set don't have a corresponding argument in `DelatunerModelForCausalLM.forward` and have been ignored: prompt_sources, prompt_targets. If prompt_sources, prompt_targets are not expected by `DelatunerModelForCausalLM.forward`,  you can safely ignore this message.
[INFO|trainer.py:3173] 2023-12-04 22:10:57,272 >> ***** Running Evaluation *****
[INFO|trainer.py:3175] 2023-12-04 22:10:57,272 >>   Num examples = 400
[INFO|trainer.py:3178] 2023-12-04 22:10:57,272 >>   Batch size = 4
[WARNING|logging.py:314] 2023-12-04 22:10:57,276 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:24,  4.01it/s]  3%|▎         | 3/100 [00:01<00:36,  2.63it/s]  4%|▍         | 4/100 [00:01<00:45,  2.11it/s]  5%|▌         | 5/100 [00:02<00:50,  1.90it/s]  6%|▌         | 6/100 [00:03<00:55,  1.69it/s]  7%|▋         | 7/100 [00:03<00:56,  1.65it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  9%|▉         | 9/100 [00:04<00:51,  1.78it/s] 10%|█         | 10/100 [00:05<00:52,  1.71it/s] 11%|█         | 11/100 [00:05<00:52,  1.68it/s] 12%|█▏        | 12/100 [00:06<00:54,  1.60it/s] 13%|█▎        | 13/100 [00:07<00:54,  1.59it/s] 14%|█▍        | 14/100 [00:07<00:53,  1.62it/s] 15%|█▌        | 15/100 [00:08<00:51,  1.65it/s] 16%|█▌        | 16/100 [00:09<00:49,  1.70it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:44,  1.83it/s] 19%|█▉        | 19/100 [00:10<00:43,  1.88it/s] 20%|██        | 20/100 [00:11<00:43,  1.84it/s] 21%|██        | 21/100 [00:11<00:43,  1.81it/s] 22%|██▏       | 22/100 [00:12<00:42,  1.84it/s] 23%|██▎       | 23/100 [00:12<00:41,  1.84it/s] 24%|██▍       | 24/100 [00:13<00:41,  1.84it/s] 25%|██▌       | 25/100 [00:13<00:40,  1.84it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:43,  1.70it/s] 28%|██▊       | 28/100 [00:15<00:42,  1.68it/s] 29%|██▉       | 29/100 [00:16<00:41,  1.72it/s] 30%|███       | 30/100 [00:16<00:39,  1.75it/s] 31%|███       | 31/100 [00:17<00:39,  1.75it/s] 32%|███▏      | 32/100 [00:18<00:40,  1.68it/s] 33%|███▎      | 33/100 [00:18<00:38,  1.72it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.85it/s] 36%|███▌      | 36/100 [00:20<00:33,  1.89it/s] 37%|███▋      | 37/100 [00:20<00:33,  1.90it/s] 38%|███▊      | 38/100 [00:21<00:33,  1.86it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.84it/s] 40%|████      | 40/100 [00:22<00:32,  1.86it/s] 41%|████      | 41/100 [00:22<00:31,  1.87it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 43%|████▎     | 43/100 [00:24<00:33,  1.72it/s] 44%|████▍     | 44/100 [00:24<00:32,  1.71it/s] 45%|████▌     | 45/100 [00:25<00:33,  1.65it/s] 46%|████▌     | 46/100 [00:25<00:33,  1.62it/s] 47%|████▋     | 47/100 [00:26<00:33,  1.60it/s] 48%|████▊     | 48/100 [00:27<00:33,  1.56it/s] 49%|████▉     | 49/100 [00:27<00:33,  1.50it/s] 50%|█████     | 50/100 [00:28<00:33,  1.50it/s] 51%|█████     | 51/100 [00:29<00:31,  1.55it/s] 52%|█████▏    | 52/100 [00:29<00:30,  1.58it/s] 53%|█████▎    | 53/100 [00:30<00:28,  1.63it/s] 54%|█████▍    | 54/100 [00:30<00:27,  1.67it/s] 55%|█████▌    | 55/100 [00:31<00:26,  1.70it/s] 56%|█████▌    | 56/100 [00:32<00:25,  1.71it/s] 57%|█████▋    | 57/100 [00:32<00:24,  1.73it/s] 58%|█████▊    | 58/100 [00:33<00:24,  1.73it/s] 59%|█████▉    | 59/100 [00:33<00:23,  1.72it/s] 60%|██████    | 60/100 [00:34<00:23,  1.71it/s] 61%|██████    | 61/100 [00:34<00:22,  1.71it/s] 62%|██████▏   | 62/100 [00:35<00:22,  1.70it/s] 63%|██████▎   | 63/100 [00:36<00:21,  1.73it/s] 64%|██████▍   | 64/100 [00:36<00:22,  1.63it/s] 65%|██████▌   | 65/100 [00:37<00:22,  1.54it/s] 66%|██████▌   | 66/100 [00:38<00:22,  1.51it/s] 67%|██████▋   | 67/100 [00:38<00:22,  1.49it/s] 68%|██████▊   | 68/100 [00:39<00:21,  1.52it/s] 69%|██████▉   | 69/100 [00:40<00:20,  1.53it/s] 70%|███████   | 70/100 [00:40<00:20,  1.46it/s] 71%|███████   | 71/100 [00:41<00:20,  1.41it/s] 72%|███████▏  | 72/100 [00:42<00:19,  1.42it/s] 73%|███████▎  | 73/100 [00:43<00:19,  1.36it/s] 74%|███████▍  | 74/100 [00:43<00:19,  1.36it/s] 75%|███████▌  | 75/100 [00:44<00:17,  1.42it/s] 76%|███████▌  | 76/100 [00:45<00:15,  1.53it/s] 77%|███████▋  | 77/100 [00:45<00:14,  1.56it/s] 78%|███████▊  | 78/100 [00:46<00:14,  1.55it/s] 79%|███████▉  | 79/100 [00:47<00:14,  1.49it/s] 80%|████████  | 80/100 [00:47<00:14,  1.40it/s] 81%|████████  | 81/100 [00:48<00:13,  1.41it/s] 82%|████████▏ | 82/100 [00:49<00:12,  1.46it/s] 83%|████████▎ | 83/100 [00:49<00:11,  1.47it/s] 84%|████████▍ | 84/100 [00:50<00:11,  1.42it/s] 85%|████████▌ | 85/100 [00:51<00:10,  1.40it/s] 86%|████████▌ | 86/100 [00:52<00:10,  1.36it/s] 87%|████████▋ | 87/100 [00:53<00:09,  1.35it/s] 88%|████████▊ | 88/100 [00:53<00:08,  1.42it/s] 89%|████████▉ | 89/100 [00:54<00:07,  1.42it/s] 90%|█████████ | 90/100 [00:55<00:07,  1.36it/s] 91%|█████████ | 91/100 [00:55<00:06,  1.39it/s] 92%|█████████▏| 92/100 [00:56<00:05,  1.50it/s] 93%|█████████▎| 93/100 [00:57<00:04,  1.48it/s] 94%|█████████▍| 94/100 [00:57<00:04,  1.43it/s] 95%|█████████▌| 95/100 [00:58<00:03,  1.47it/s] 96%|█████████▌| 96/100 [00:59<00:02,  1.50it/s] 97%|█████████▋| 97/100 [00:59<00:02,  1.47it/s] 98%|█████████▊| 98/100 [01:00<00:01,  1.49it/s] 99%|█████████▉| 99/100 [01:01<00:00,  1.48it/s]100%|██████████| 100/100 [01:01<00:00,  1.46it/s]100%|██████████| 100/100 [01:02<00:00,  1.61it/s]
Map:   0%|          | 0/400 [00:00<?, ? examples/s]Map: 100%|██████████| 400/400 [00:00<00:00, 9156.12 examples/s]
***** eval metrics *****
  eval_loss               =     0.9391
  eval_runtime            = 0:01:02.86
  eval_samples            =        400
  eval_samples_per_second =      6.362
  eval_steps_per_second   =      1.591
  eval_tokens             =      39160
2023-12-04 22:12:05.747161: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-04 22:12:05.747209: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-04 22:12:05.747234: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-04 22:12:06.498973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
finetune_args is 
 FinetuneArguments(lora_rank=8, lora_alpha=16, lora_dropout=0.05, lora_target_modules=None, adapter_layers=30, adapter_len=10, num_virtual_tokens=10, ptun_hidden_size=1024, peft='lora', resume_peft='/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora', delta='lora', profile=False, train_on_inputs=True, habana=False, debugs=False, save_merged_model=True, merge_model_code_dir='', load_in_8bit=False, input_sentence='', output_length_limit=20)
2023-12-04 22:12:07,466 - __main__ - WARNING - Process rank: 0, device: cuda:0
distributed training: True, 16-bits training: True
2023-12-04 22:12:07,467 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora/runs/Dec04_22-12-07_vsr134,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'codecarbon'],
resume_from_checkpoint=None,
run_name=/home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:715] 2023-12-04 22:12:07,468 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:777] 2023-12-04 22:12:07,468 >> Model config LlamaConfig {
  "_name_or_path": "/home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.36.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:12:07,470 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:12:07,470 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:12:07,470 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:12:07,470 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2023-12-04 22:12:07,470 >> loading file tokenizer_config.json
/opt/conda/lib/python3.10/site-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:3118] 2023-12-04 22:12:09,213 >> loading weights file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1222] 2023-12-04 22:12:09,213 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:796] 2023-12-04 22:12:09,214 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.39it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.75it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.69it/s]
[INFO|modeling_utils.py:3950] 2023-12-04 22:12:09,938 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3958] 2023-12-04 22:12:09,939 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:749] 2023-12-04 22:12:09,941 >> loading configuration file /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:796] 2023-12-04 22:12:09,941 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

Map:   0%|          | 0/1423 [00:00<?, ? examples/s]Map:  70%|███████   | 1000/1423 [00:00<00:00, 8114.11 examples/s]Map: 100%|██████████| 1423/1423 [00:00<00:00, 7925.30 examples/s]
2023-12-04 22:12:10,158 - __main__ - INFO - Using data collator of type DataCollatorForSeq2Seq
2023-12-04 22:12:24,617 - __main__ - INFO - ***original optimized model parameter***
trainable params: 0 || all params: 6,742,609,920 || trainable%: 0.0
2023-12-04 22:12:27,625 - __main__ - INFO - ***deltatuner optimized model parameter***
trainable params: 0 || all params: 6,740,791,296 || trainable%: 0.0
[INFO|trainer.py:593] 2023-12-04 22:12:31,656 >> Using auto half precision backend
/opt/conda/lib/python3.10/site-packages/tzlocal/unix.py:193: UserWarning: Can not find any timezone configuration, defaulting to UTC.
  warnings.warn("Can not find any timezone configuration, defaulting to UTC.")
2023-12-04 22:12:31,661 - apscheduler.scheduler - INFO - Adding job tentatively -- it will be properly scheduled when the scheduler starts
copy base model config to /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora/merged_model
cp: -r not specified; omitting directory '/home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf/analysis'
remove unnecessary file from /home/vmagent/app/LLM_datapre/data//Llama-2-7b-hf
Save merged model to /home/vmagent/app/LLM_datapre/data//QA_dataset/report_qa_nomax/models/Llama-2-7b-hf_denas-lora/merged_model
